{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa7fdb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d896e74",
   "metadata": {},
   "source": [
    "# 1. Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a2701ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(number_of_layers, layers_dims):\n",
    "    \"\"\"\n",
    "    Generate dictionary of parameters (weights and biases) for each layer of NN.\n",
    "        - Weight matrices will contain small random number\n",
    "        - Bias vectors will contain zeros.\n",
    "    \n",
    "    Argumetns:\n",
    "    number_of_layers -- integer specifying number of layers in NN; L\n",
    "    layers_dims -- 1D numpy array, containing number of units (integer) in each layer of NN, including input layer; from layer 0 to L\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary with parameter matrices 'W' and vectors 'b' for each layer of NN, from 1 to L:\n",
    "                Wl - weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                bl - bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    params = {}\n",
    "    \n",
    "    for l in range(1, number_of_layers+1):\n",
    "        params['W'+str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) *0.01\n",
    "        params['b'+str(l)] = np.zeros((layers_dims[l], 1))\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c8dabb",
   "metadata": {},
   "source": [
    "# 2. Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80634289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Compute Sigmoid function for matrix Z.\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- 2D numpy array, containing linear transformation of previous layer post-activation matrix.\n",
    "        Z = WA_prev + b\n",
    "        shape: (size of current layer, number of examples)  --> (layer_dims[l], m)\n",
    "    \n",
    "    Returns:\n",
    "    A -- 2D numpy array; Sigmoid function of Z:\n",
    "        A = 1 / (1 + e^(-Z))\n",
    "        shape is the same as Z: (layer_dims[l], m)\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45e8dea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(Z):\n",
    "    \"\"\"\n",
    "    Compute Tanh function for matrix Z.\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- 2D numpy array, containing linear transformation of previous layer post-activation matrix.\n",
    "        Z = WA_prev + b\n",
    "        shape: (size of current layer, number of examples)  --> (layer_dims[l], m)\n",
    "    \n",
    "    Returns:\n",
    "    A -- 2D numpy array; Tanh function of Z:\n",
    "        A = (e^Z - e^(-Z)) / (e^Z + e^(-Z)) \n",
    "        shape is the same as Z: (layer_dims[l], m)\n",
    "    \"\"\"\n",
    "    \n",
    "    A = (np.exp(Z) - np.exp(-Z)) / (np.exp(Z) + np.exp(-Z))\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2da09462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Compute ReLU function for matrix Z.\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- 2D numpy array, containing linear transformation of previous layer post-activation matrix.\n",
    "        Z = WA_prev + b\n",
    "        shape: (size of current layer, number of examples)  --> (layer_dims[l], m)\n",
    "    \n",
    "    Returns:\n",
    "    A -- 2D numpy array; ReLU function of Z:\n",
    "        A = max(0, Z)\n",
    "        shape is the same as Z: (layer_dims[l], m)\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0, Z)\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81659bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(Z):\n",
    "    \"\"\"\n",
    "    Compute Leaky ReLU function for matrix Z.\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- 2D numpy array, containing linear transformation of previous layer post-activation matrix.\n",
    "        Z = WA_prev + b\n",
    "        shape: (size of current layer, number of examples)  --> (layer_dims[l], m)\n",
    "    \n",
    "    Returns:\n",
    "    A -- 2D numpy array; Leaky ReLU function of Z\n",
    "        shape is the same as Z: (layer_dims[l], m)\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0.01*Z, Z)\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3855b9",
   "metadata": {},
   "source": [
    "# 3. Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d5d512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Compute derivative of sigmoid function wrt matrix Z.\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- 2D numpy array, containing linear transformation of previous layer post-activation matrix.\n",
    "        Z = WA_prev + b\n",
    "        shape: (size of current layer, number of examples)  --> (layer_dims[l], m)\n",
    "    \n",
    "    Returns:\n",
    "    g_prim -- 2D numpy array; derivative of sigmoid function of Z:\n",
    "        g_prim = g(Z) * (1-g(Z))\n",
    "        shape is the same as Z: (layer_dims[l], m)\n",
    "    \"\"\"\n",
    "    \n",
    "    g_prim = sigmoid(Z) * (1 - sigmoid(Z))\n",
    "    \n",
    "    return g_prim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e529bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_tanh(Z):\n",
    "    \"\"\"\n",
    "    Compute derivative of tanh function wrt matrix Z.\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- 2D numpy array, containing linear transformation of previous layer post-activation matrix.\n",
    "        Z = WA_prev + b\n",
    "        shape: (size of current layer, number of examples)  --> (layer_dims[l], m)\n",
    "    \n",
    "    Returns:\n",
    "    g_prim -- 2D numpy array; derivative of tanh function of Z:\n",
    "        g_prim = 1 - tanh(Z)^2 \n",
    "        shape is the same as Z: (layer_dims[l], m)\n",
    "    \"\"\"\n",
    "    \n",
    "    g_prim = 1 - np.power(tanh(Z), 2)\n",
    "    \n",
    "    return g_prim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdb11a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_relu(Z):\n",
    "    \"\"\"\n",
    "    Compute derivative of ReLU function wrt matrix Z.\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- 2D numpy array, containing linear transformation of previous layer post-activation matrix.\n",
    "        Z = WA_prev + b\n",
    "        shape: (size of current layer, number of examples)  --> (layer_dims[l], m)\n",
    "    \n",
    "    Returns:\n",
    "    g_prim -- 2D numpy array; derivative of ReLU function of Z:\n",
    "        g_prim = {\n",
    "            0 if Z < 0\n",
    "            1 if Z >= 0\n",
    "        }\n",
    "        shape is the same as Z: (layer_dims[l], m)\n",
    "    \"\"\"\n",
    "    \n",
    "    g_prim = (Z >= 0) * 1  # Boolean matrix multiplied by 1\n",
    "    \n",
    "    return g_prim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58f65965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_leaky_relu(Z):\n",
    "    \"\"\"\n",
    "    Compute derivative of Leaky ReLU function wrt matrix Z.\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- 2D numpy array, containing linear transformation of previous layer post-activation matrix.\n",
    "        Z = WA_prev + b\n",
    "        shape: (size of current layer, number of examples)  --> (layer_dims[l], m)\n",
    "    \n",
    "    Returns:\n",
    "    g_prim -- 2D numpy array; derivative of Leaky ReLU function of Z:\n",
    "        g_prim = {\n",
    "            0.01   if Z < 0\n",
    "            1      if Z >= 0\n",
    "        }\n",
    "        shape is the same as Z: (layer_dims[l], m)\n",
    "    \"\"\"\n",
    "    \n",
    "    g_prim = (Z >= 0) * 0.99 + 0.01  # It will give 0.01 to all and add 0.99 for positive Z\n",
    "    \n",
    "    return g_prim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84764e6d",
   "metadata": {},
   "source": [
    "# 4. Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b6147df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, number_of_layers, layers_dims, params):\n",
    "    \"\"\"\n",
    "    Perform a full forward propagation process.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- 2D numpy array;\n",
    "    architecture -- tuple; \n",
    "    \n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    \n",
    "#     # Initialize parameters\n",
    "#     params = initialize_parameters(number_of_layers, layers_dims)\n",
    "    \n",
    "    # Create dictionaries to store values of Z and A for each layer\n",
    "    Z_dict = {}\n",
    "    A_dict = {\"A0\": X}\n",
    "    \n",
    "    # Perform forward steps for layers 1, ..., L-1\n",
    "    for l in range(1, number_of_layers):\n",
    "        Z_dict[\"Z\"+str(l)] = np.dot(params[\"W\"+str(l)], A_dict[\"A\"+str(l-1)]) + params[\"b\"+str(l)]\n",
    "        A_dict[\"A\"+str(l)] = relu(Z_dict[\"Z\"+str(l)])   # ReLU activation\n",
    "        \n",
    "    L = number_of_layers\n",
    "    Z_dict[\"Z\"+str(L)] = np.dot(params[\"W\"+str(L)], A_dict[\"A\"+str(L-1)]) + params[\"b\"+str(L)]\n",
    "    A_dict[\"A\"+str(L)] = sigmoid(Z_dict[\"Z\"+str(L)])   # sigmoid activation\n",
    "    \n",
    "    return Z_dict, A_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fc071a",
   "metadata": {},
   "source": [
    "# 5. Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecc8f83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Y, AL):\n",
    "    \"\"\"\n",
    "    Computes cost function - function to measure how good is the fit.\n",
    "    \n",
    "    Arguments:\n",
    "    Y -- vector of true labels, shape (1, number of examples)\n",
    "    AL - probability vector corresponding to predicted labels, shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    cost -- float number; cross-entropy cost\"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    cost = -1/m * (np.dot(Y, np.log(AL).T) + np.dot(1-Y, np.log(1-AL).T))\n",
    "    \n",
    "    cost = np.squeeze(cost)    # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    \n",
    "    return cost               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3830125c",
   "metadata": {},
   "source": [
    "# 6. Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cabda6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(Y, Z_dict, A_dict, number_of_layers, params):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Y, Z_dict, A_dict, number_of_layers, params\n",
    "    \n",
    "    Returns:\n",
    "    grads - dictionary of gradients\n",
    "    \"\"\"\n",
    "    L = number_of_layers\n",
    "    grads = {}\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    ###\n",
    "    AL = A_prev = A_dict[\"A\"+str(L)]\n",
    "    ZL = Z_dict[\"Z\"+str(L)]\n",
    "    A_prev = A_dict[\"A\"+str(L-1)]\n",
    "    WL = params[\"W\"+str(L)]\n",
    "    \n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    dZL = np.multiply(dAL, d_sigmoid(ZL))       # sigmoid activation\n",
    "    dWL = 1/m * np.dot(dZL, A_prev.T)\n",
    "    dbL = 1/m * np.sum(dZL, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(WL.T, dZL)\n",
    "    \n",
    "    grads[\"dW\" + str(L)] = dWL\n",
    "    grads[\"db\" + str(L)] = dbL\n",
    "    \n",
    "    for l in reversed(range(1, L)):\n",
    "        Zl = Z_dict[\"Z\"+str(l)]\n",
    "        A_prev = A_dict[\"A\"+str(l-1)]\n",
    "        Wl = params[\"W\"+str(l)]\n",
    "\n",
    "        dZl = np.multiply(dA_prev, d_relu(Zl))    # ReLU activation\n",
    "        dWl = 1/m * np.dot(dZl, A_prev.T)\n",
    "        dbl = 1/m * np.sum(dZl, axis=1, keepdims=True)\n",
    "        dA_prev = np.dot(Wl.T, dZl)\n",
    "\n",
    "        grads[\"dW\" + str(l)] = dWl\n",
    "        grads[\"db\" + str(l)] = dbl\n",
    "    \n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf70960d",
   "metadata": {},
   "source": [
    "# 7. Update parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5e2f2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    \n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(params) // 2\n",
    "    \n",
    "    for l in range(1, L+1):\n",
    "        params[\"W\"+str(l)] = params[\"W\"+str(l)] - learning_rate * grads[\"dW\"+str(l)]\n",
    "        params[\"b\"+str(l)] = params[\"b\"+str(l)] - learning_rate * grads[\"db\"+str(l)]\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6768d24",
   "metadata": {},
   "source": [
    "# 8. Complete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "495c5bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(X_train, Y_train, learning_rate, n_epochs):\n",
    "    \n",
    "    # Get dimensions of input data\n",
    "    n_x = X_train.shape[0]\n",
    "    n_y = Y_train.shape[0]\n",
    "    m = X_train.shape[1]\n",
    "    \n",
    "    # Get the architecture from the user\n",
    "    print(\"Determine the architecture of your neural network.\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "    \n",
    "    number_of_layers = int(\n",
    "        input(\"Specify number of layers in your neural network (number of hidden layers plus output layer): \"))\n",
    "    \n",
    "    layers_dims = []\n",
    "    layers_dims.append(n_x)\n",
    "    \n",
    "    for i in range(1, number_of_layers):\n",
    "        number_of_units = int(input(f\"Specify number of units in a layer {i}: \"))\n",
    "        layers_dims.append(number_of_units)\n",
    "    \n",
    "    layers_dims.append(n_y)\n",
    "    architecture = (number_of_layers, layers_dims)\n",
    "    \n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(f\"Your Neural Network has {number_of_layers} layers (hidden and output).\")\n",
    "    print(f\"Input layer has {layers_dims[0]} units\")\n",
    "    print(f\"Hidden layers have {layers_dims[1:-1]} units respectively.\")\n",
    "    print(f\"Output layer has {layers_dims[-1]} units.\")\n",
    "    # --------------------\n",
    "    # Train neural network\n",
    "    # --------------------\n",
    "    \n",
    "    # Initialize parameters\n",
    "    params = initialize_parameters(*architecture)\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        # Forward propagation\n",
    "        Z_dict, A_dict = forward_propagation(X_train, number_of_layers, layers_dims, params)\n",
    "        # Compute cost\n",
    "        cost = compute_cost(Y_train, A_dict[\"A\"+str(number_of_layers)])\n",
    "        # Backpropagation - Compute gradients\n",
    "        grads = backward_propagation(Y_train, Z_dict, A_dict, number_of_layers, params)\n",
    "        # Update parameters\n",
    "        params = update_parameters(params, grads, learning_rate)\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            print(\"Cost after iteration \"+str(i+1)+\": \", cost)\n",
    "    \n",
    "    print(\"Cost after iteration \"+str(n_epochs)+\": \", cost)\n",
    "    \n",
    "    return params, architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d7c101",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dffb36d-cc58-4561-a2a9-fd3272572fce",
   "metadata": {},
   "source": [
    "# Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0945b827",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.genfromtxt('x_train.csv', delimiter=',')\n",
    "X_test = np.genfromtxt('x_test.csv', delimiter=',')\n",
    "Y_train = np.genfromtxt('y_train.csv', delimiter=',')\n",
    "Y_test = np.genfromtxt('y_test.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0adbb8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2) (1000, 2) (1000,) (1000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5666afb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape \n",
    "X_train = X_train.reshape(X_train.shape[0], -1).T\n",
    "X_test = X_test.reshape(X_test.shape[0], -1).T\n",
    "Y_train = Y_train.reshape(Y_train.shape[0], -1).T\n",
    "Y_test = Y_test.reshape(Y_test.shape[0], -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e49b6dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1000) (2, 1000) (1, 1000) (1, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "108375e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = X_train.shape[0]\n",
    "m = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66c81d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determine the architecture of your neural network.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Specify number of layers in your neural network (number of hidden layers plus output layer):  3\n",
      "Specify number of units in a layer 1:  8\n",
      "Specify number of units in a layer 2:  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Your Neural Network has 3 layers (hidden and output).\n",
      "Input layer has 2 units\n",
      "Hidden layers have [8, 4] units respectively.\n",
      "Output layer has 1 units.\n",
      "Cost after iteration 1:  0.6931595464996017\n",
      "Cost after iteration 101:  0.36102888051418824\n",
      "Cost after iteration 201:  0.2651528561784696\n",
      "Cost after iteration 301:  0.02376653365386409\n",
      "Cost after iteration 401:  0.015670695007657522\n",
      "Cost after iteration 501:  0.012638561017945964\n",
      "Cost after iteration 601:  0.01101069175768086\n",
      "Cost after iteration 601:  0.01101069175768086\n"
     ]
    }
   ],
   "source": [
    "new_params, architecture = neural_network(X_train, Y_train, learning_rate=0.01, n_epochs=601)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03b65caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0020524524596953974"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict\n",
    "Z_predicted, A_predicted = forward_propagation(X_test, *architecture, new_params)\n",
    "number_of_layers = architecture[0]\n",
    "Y_hat = A_predicted[\"A\"+str(number_of_layers)]\n",
    "\n",
    "# The average difference between true label and the probabilty of label 1:\n",
    "np.sum(Y_test - Y_hat) / Y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7823ff",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
